# AI/LLM Module — Deep Code Review
**Date:** 2026-02-20
**Reviewer:** Code Reviewer Agent (code-reviewer/sonnet)
**Scope:** All 91 .kt files in `Modules/AI/LLM/src/`
**Source sets covered:** commonMain (6 files), androidMain (~75 files), desktopMain (4 files), androidUnitTest (9 files), androidInstrumentedTest (6 files)

---

## Summary

The AI/LLM module is architecturally ambitious — supporting on-device inference via TVM/MLC, GGUF/llama.cpp, and LiteRT runtimes alongside five cloud providers — but contains a high density of correctness bugs, security vulnerabilities, stub implementations, and Rule 7 (AI attribution) violations. The most critical issues are: a hardcoded absolute developer file path leaking into production code, explicit Claude/AI author attribution in three source files, an unsafe TAR extraction path-traversal vector, a KV-cache no-op that silently destroys inference performance, a `TVMModelLoader` that loads before download completes, and multiple thread-safety violations. Many of these would cause silent wrong behavior or crash at runtime before the user ever sees output.

---

## Issues

| Severity | File:Line | Issue | Suggestion |
|----------|-----------|-------|------------|
| **CRITICAL** | `androidMain/kotlin/com/augmentalis/llm/ModelSelector.kt:46,58,102,112` | Absolute developer machine paths (`/Users/manoj_mbpm14/Downloads/Coding/MLC-LLM-Code/...`) hardcoded in `localSourcePath` fields of `ModelSpec` entries. These paths are embedded in the production `companion object` and will be shipped in the release APK. | Remove `localSourcePath` from all production `ModelSpec` entries, or gate behind a `BuildConfig.DEBUG` flag and set via developer-only companion config. |
| **CRITICAL** | `androidMain/kotlin/com/augmentalis/llm/alc/loader/ALMExtractor.kt:230` | Path traversal in TAR extraction: `File(destDir, entry.name)` is constructed without validating that `entry.name` does not contain `../` sequences. A malicious `.amm/.amg/.amr` archive could write files to arbitrary locations on the device filesystem. | Validate each entry name: `require(!entry.name.contains(".."))` and use `canonicalPath.startsWith(destDir.canonicalPath)` before writing. |
| **CRITICAL** | `androidMain/kotlin/com/augmentalis/llm/response/TemplateResponseGenerator.kt:25` | Explicit Rule 7 violation: file header reads `Author: Claude Code (Agent 3)`. This is an absolute prohibition — no AI identity may appear anywhere in any file. | Replace with `Author: Manoj Jhawar` or remove the author line. |
| **CRITICAL** | `androidMain/kotlin/com/augmentalis/llm/response/LLMContextBuilder.kt:18` | Explicit Rule 7 violation: file header reads `Author: Claude Code (Agent 3)`. | Replace with `Author: Manoj Jhawar` or remove the author line. |
| **CRITICAL** | `commonMain/kotlin/com/augmentalis/llm/CommandInterpretation.kt:10-11` | Explicit Rule 7 violation: file header reads `Author: Claude (VoiceOSCore AI Integration Phase 2)`. | Replace with `Author: Manoj Jhawar` or remove the author line. |
| **CRITICAL** | `androidMain/kotlin/com/augmentalis/llm/alc/streaming/BackpressureStreamingManager.kt:323-338` | `IMemoryManager.getCache()` and `setCache()` are declared as extension functions with bodies that are complete no-ops: `getCache()` returns `null` unconditionally (reads from a stats map key that is never set); `setCache()` does nothing. The comment explicitly says "In full implementation, this would call a proper method." This means the KV cache is never passed between tokens — every generation step starts from scratch, inflating latency by O(context_length) per token. | Implement real `IMemoryManager` KV cache passthrough or elevate this to a tracked stub that blocks production release. |
| **CRITICAL** | `androidMain/kotlin/com/augmentalis/llm/alc/loader/TVMModelLoader.kt:62` | `downloader.downloadModel(downloadConfig).first()` collects only the **first** emission from the download Flow (which is a `DownloadState.Downloading` progress event, not completion). The loader then immediately proceeds to open the model file before the download has completed, causing a "file not found" or corrupt-read at inference time. | Replace `.first()` with `.last()` or collect with a predicate: `.first { it is DownloadState.Completed || it is DownloadState.Error }`, then check for completion before proceeding. |
| **CRITICAL** | `androidMain/kotlin/com/augmentalis/llm/provider/GoogleAIProvider.kt:318` | API key is appended to the HTTP URL as a query parameter (`?key=$apiKey`). Query parameters are logged by Android's `HttpURLConnection`, OkHttp interceptors, server-side access logs, and any network proxy. This leaks the Google AI API key to anyone with access to those logs. | Move the API key to the `x-goog-api-key` request header. |
| **CRITICAL** | `androidMain/kotlin/com/augmentalis/llm/provider/OllamaProvider.kt` (model name interpolation) | `pullModel` and related endpoints interpolate the model name directly into a JSON string without escaping. A model name containing `"` or `\n` will produce malformed JSON and can be used for JSON injection if model IDs are ever sourced from external input. | Use `kotlinx.serialization` to serialize the JSON body, never string-interpolate into JSON. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/alc/TVMRuntime.kt:390-391` | `dispose()` has an explicit stub comment: `// Stub - will implement when full TVM integration is done`. Native TVM resources (JNI handles, GPU memory) are never released. | Implement proper JNI cleanup: release module handles via `tvm_runtime_destroy` or equivalent, set all handles to 0 after release. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/alc/inference/GGUFInferenceStrategy.kt` | Companion object holds `@Volatile` static native pointers (`nativeContextPtr`, `nativeModelPtr`). All instances of `GGUFInferenceStrategy` share the same JNI handle, meaning creating a second instance (e.g., for model hot-swap) will silently corrupt the first. | Move native pointers to instance fields; use a factory or singleton pattern to enforce single-instance. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/inference/InferenceManager.kt:182-185` | `registerReceiver` call for battery broadcast does not pass `RECEIVER_EXPORTED` or `RECEIVER_NOT_EXPORTED` flag. On API 34+ (Android 14+) this throws `SecurityException` at runtime, crashing the inference manager at initialization. | Add `ContextCompat.RECEIVER_NOT_EXPORTED` as the flags argument. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/alc/ALCEngineSingleLanguage.kt:153-154` | The streaming loop always emits `ResponseChunk.Complete(fullText = "")` — the `fullText` field is hardcoded to an empty string. Any consumer that reads `complete.fullText` (rather than assembling chunks) gets an empty response. | Accumulate the text in a `StringBuilder` across `ResponseChunk.Text` emissions and pass the assembled string to `ResponseChunk.Complete`. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/provider/HuggingFaceProvider.kt:387-390` | `generateResponse()` buffers the complete HTTP response body, then artificially chunks it by splitting on spaces. This is **fake streaming** — it does not reduce time-to-first-token or memory pressure; the full response is already in memory. | Either implement real SSE streaming (as OpenAI/Anthropic providers do) or honestly return a single `LLMResponse.Complete` without pretending to stream. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/response/HybridResponseGenerator.kt` | `HybridResponseGenerator` implements the androidMain `ResponseGenerator` interface (signature: `generateResponse(userMessage, IntentClassification, context)`), but the commonMain `ResponseGenerator` interface signature is `generateResponse(userMessage, intent: String, confidence: Float, context)`. These interfaces are incompatible. Hilt will inject one where the other is expected, resulting in a `ClassCastException` or compile error depending on which interface is resolved. | Unify the two `ResponseGenerator` interfaces into a single commonMain definition; remove the androidMain-specific override. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/download/LLMModelDownloader.kt:274-278` | `observeDownloadProgress()` has an empty body — it subscribes to nothing and emits nothing. Any caller that relies on this for progress UI will see no updates. | Implement by collecting from `WorkManager.getWorkInfoByIdFlow(downloadWorkId)` and mapping the `progress` `Data` object to the emitted `DownloadState`. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/provider/CloudLLMProvider.kt:134` | `var isGenerating: Boolean` (plain non-atomic) is read from the UI thread via `isCurrentlyGenerating()` and written from the IO coroutine in `generateResponse()`. This is a data race. | Replace with `@Volatile var isGenerating` or use `AtomicBoolean`. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/download/ModelDownloadManager.kt:547` | `DownloadJob` data class has mutable `var` fields (`bytesDownloaded`, `isPaused`, `isCancelled`) accessed from both the download coroutine loop and the `pauseDownload()`/`cancelDownload()` callers without holding `downloadMutex`. Concurrent writes produce data races; `isCancelled` may be read stale in the download loop. | Change to `@Volatile var isCancelled` and `@Volatile var isPaused`; or move state into an `AtomicBoolean` pair. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/alc/inference/MLCInferenceStrategy.kt:55` | `isAvailable()` always returns `true` with a `// TODO` comment. The `MultiProviderInferenceStrategy` uses this to filter available providers. MLC will always appear available, even on devices without the TVM native library, causing every inference attempt to crash via JNI. | Check for the native library presence: `return try { System.loadLibrary("tvm4j_core"); true } catch (e: UnsatisfiedLinkError) { false }`. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/security/ApiKeyManager.kt` | API keys are stored in `EncryptedSharedPreferences` (AES-256 GCM) — this is correct. However, `getApiKey()` returns the raw key string directly to callers outside the module. Any caller can then log, serialize, or transmit the key. | Return an opaque handle or inject the key directly into OkHttp interceptors that are scoped to this module; callers should never hold the raw key string. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/download/ChecksumHelper.kt:133,145,157` | All three `KnownChecksums` entries (`GEMMA_2B_IT`, `GEMMA_2B_INSTRUCT`, `MOBILE_BERT`) have checksum value `"TODO_GENERATE_AFTER_DOWNLOAD"`. The checksum verification infrastructure exists and is called, but it will always fail for these models (or is skipped because the caller checks for non-null/non-empty). No model download is actually verified. | Generate real SHA-256 checksums for the published model files and populate these constants before enabling downloads in production. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/download/ModelDownloadConfig.kt:178-180` | `FirebaseStorage.getDownloadUrl()` returns the literal string `"firebase://bucket/path"` — a placeholder URI, not a valid HTTPS URL. Any download attempt using `DownloadSource.FirebaseStorage` will fail with an `MalformedURLException` or HTTP error. | Integrate the Firebase Storage SDK and call `storageRef.downloadUrl.await().toString()` for a real signed HTTPS URL. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/alc/inference/LiteRTInferenceStrategy.kt:94` | `vocabSize` is hardcoded to `256000` regardless of the model loaded. Gemma, Llama, and Qwen models have different vocabulary sizes. Incorrect vocab size causes wrong logit interpretation during sampling. | Load `vocabSize` from the model's config JSON (`model_config.json` or `ava-model-config.json`) at initialization. |
| **HIGH** | `androidMain/kotlin/com/augmentalis/llm/alc/loader/ModelDownloader.kt:214-241` | `listAvailableModels()` returns `emptyList()` unconditionally — the real implementation is commented out. Any UI showing "available models to download" is broken. | Either implement the catalog fetch or remove the method from the public API; do not ship a public method that silently returns empty. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/download/LLMDownloadWorker.kt:98-101` | `GlobalScope.launch {}` is used inside a `CoroutineWorker` to call `setForeground()` during progress updates. This detaches the coroutine from WorkManager's lifecycle. If the worker is cancelled, the `GlobalScope` coroutine continues running and may call `setForeground()` on a dead worker, throwing `IllegalStateException`. | WorkManager's `doWork()` is itself a suspend function — `setForeground()` can be called directly from the progress lambda if you hoist the lambda to a suspend callback, or use `coroutineScope { launch { ... } }` instead of `GlobalScope`. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/alc/samplers/TopPSampler.kt:39` | `exp(it.toDouble())` is computed directly on raw logit values. For models with logits in the range [−10, 10+], large logits cause `exp()` to overflow to `Infinity`, which then propagates `NaN` into the probability distribution and causes undefined sampling behavior. | Subtract the maximum logit before exponentiating: `val maxLogit = logits.max(); val exps = logits.map { exp((it - maxLogit).toDouble()) }`. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/alc/language/LanguagePackManager.kt:263` | `getDownloadProgress()` returns `0f` unconditionally — progress tracking is not implemented. The `downloadProgressCallbacks` map is also a plain `mutableMapOf` accessed from IO coroutines without synchronization, which can cause `ConcurrentModificationException`. | Replace the callback map with `ConcurrentHashMap`; implement `getDownloadProgress()` by reading from an active `downloadFileWithProgress()` coroutine's state. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/alc/tokenizer/TVMTokenizer.kt` | `encodeCache` and `decodeCache` are plain `mutableMapOf<>()` instances. `TVMTokenizer` is accessed from background coroutines (Dispatchers.IO). Concurrent puts to a non-thread-safe map produce `ConcurrentModificationException` and can corrupt the map's internal state. | Replace both caches with `Collections.synchronizedMap(LinkedHashMap(...))` or use `ConcurrentHashMap` with a max-entries eviction strategy. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/ModelSelector.kt:152` | `selectBestModel()` calls `runBlocking { ... }` inside what may already be a coroutine context (it is called from `LocalLLMProvider.initialize()`). `runBlocking` inside a coroutine can deadlock if the calling dispatcher has a limited thread pool (e.g., Dispatchers.IO with all threads busy). | Make `selectBestModel()` a `suspend` function and remove `runBlocking`. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/download/ModelStorageManager.kt:49` | `Environment.getExternalStorageDirectory()` is deprecated since API 29 and does not respect scoped storage (API 30+). On Android 10+ devices without `MANAGE_EXTERNAL_STORAGE`, this path is not writable. | Replace with `context.getExternalFilesDir(null)` which does not require the `MANAGE_EXTERNAL_STORAGE` permission and is always writable by the app. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/alc/memory/PagedKVCacheManager.kt` | Mixed concurrency primitives: `ConcurrentLinkedDeque` for the free list, but `pageTable` and `allocationMap` are plain `mutableMapOf()` accessed inside `Mutex`-protected blocks. The asymmetry makes reasoning about safety difficult, and any path that accesses `pageTable` outside `mutex.withLock` (even for reads) causes a race. | Consolidate: use `Mutex` consistently for all mutable state, or use `ConcurrentHashMap` for all maps. Avoid mixing lock-free and mutex-protected structures for the same logical operation. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/cache/TokenCacheManager.kt` | Uses MD5 to hash cache keys (cache invalidation on vocab file change). MD5 is cryptographically broken, but the more relevant concern here is that the Java implementation `MessageDigest.getInstance("MD5")` is not available on FIPS-compliant devices, which will throw `NoSuchAlgorithmException`. MD5 also has a non-trivial collision rate for short strings. | For a non-security cache key, MD5 is low-risk but should be replaced with `MurmurHash3` or `SHA-256.substring(0,8)` for FIPS compatibility. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/alc/tokenizer/HuggingFaceTokenizer.kt` | Uses MD5 for binary cache file invalidation (same concern as `TokenCacheManager`). On FIPS devices this will crash tokenizer initialization. | Same fix: use SHA-256 truncated or a non-cryptographic hash. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/download/HuggingFaceClient.kt` | All predefined model entries in `HuggingFaceClient` pass `checksum = null`. The `verifyChecksum()` method in `HuggingFaceModelDownloader` is therefore never called for any bundled model, giving false confidence that integrity is checked. | Populate SHA-256 checksums for all bundled model metadata entries. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/domain/TypeAliases.kt` | Entire file consists only of type aliases (e.g., `typealias LLMCallback = (LLMResponse) -> Unit`). This adds an indirection layer with no behavioral difference (Rule 2 violation). Any call site using these aliases is harder to navigate in an IDE than using the direct type. | Remove the aliases and use the underlying types directly at call sites. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/alc/inference/MultiProviderInferenceStrategy.kt:37` | `providers.sortedBy { it.getPriority() }` returns a new sorted list but the result is discarded — the original `providers` list remains unsorted. Fallback order is determined by insertion order, not priority. | Sort in `init`: `private val sortedProviders = providers.sortedBy { it.getPriority() }` and use `sortedProviders` in `infer()`. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/alc/language/LocalizedLanguageFilter.kt:124-127` | `getLocalizationCompleteness()` returns a hardcoded `100` for any "supported" language (only English) and `0` for all others. The method is documented as "check actual string resources" but never does. Any UI showing translation completeness percentage is misleading. | Implement via Android's `Resources.getIdentifier()` count, or remove the method from the public API. |
| **MEDIUM** | `androidMain/kotlin/com/augmentalis/llm/alc/loader/ModelDownloader.kt:56-57` | `BASE_DOWNLOAD_URL` points to `augmentalis/ava-models` HuggingFace repo, which is noted in a comment as not yet existing. `isModelAvailableForDownload()` will always return `false` and download will always fail with HTTP 404. | Create the HuggingFace repository or switch `BASE_DOWNLOAD_URL` to a working model distribution endpoint before enabling downloads in production. |
| **LOW** | `androidMain/kotlin/com/augmentalis/llm/alc/TVMRuntime.kt:98-104` | `TVMRuntime.create()` uses reflection to find `InferenceBackendSelector`. If the class is not found (ProGuard rename, missing dependency), it silently falls back to OpenCL backend without logging the specific exception. Silent fallback hides configuration problems. | Log the caught `ClassNotFoundException` as a warning before falling back. |
| **LOW** | Multiple files (20+): `OpenAIProvider.kt`, `AnthropicProvider.kt`, `GoogleAIProvider.kt`, `HuggingFaceProvider.kt`, `LocalLLMProvider.kt`, `CloudLLMProvider.kt`, `PagedKVCacheManager.kt`, `BackpressureStreamingManager.kt`, `TVMRuntime.kt`, `ALCEngine.kt`, `ALCEngineSingleLanguage.kt`, `LLMDownloadWorker.kt`, `ApiKeyManager.kt`, `LatencyMetrics.kt`, `ModelStorageManager.kt`, `ModelDownloadManager.kt`, `ChecksumHelper.kt`, `ModelDownloadConfig.kt`, `LanguageDetector.kt`, `SystemPromptManager.kt`, `ModelSelector.kt`, `HuggingFaceModelDownloader.kt`, `DownloadState.kt` | Rule 7 violation: `Author: AVA AI Team` header in file kdoc. This is prohibited by Rule 7 — no AI identity may appear as an author. | Replace `Author: AVA AI Team` with `Author: Manoj Jhawar` or remove the author line entirely from all affected file headers. |
| **LOW** | `androidMain/kotlin/com/augmentalis/llm/metrics/LatencyMetrics.kt` | File header reads `Author: AVA AI Team (YOLO Mode Implementation)` — the `(YOLO Mode Implementation)` suffix makes the AI attribution more explicit. | Same as above: replace or remove. |
| **LOW** | `androidMain/kotlin/com/augmentalis/llm/response/LLMResponseGenerator.kt` | Contains `// TODO: Implement context truncation if needed` comment. Without truncation, prompts exceeding the model's context window are passed as-is, causing undefined behavior in the inference backend (typically silent truncation or an exception). | Either implement sliding-window truncation using `LLMContextBuilder.estimateTokens()` and the model's `maxTokens` config, or promote this to a tracked issue. |
| **LOW** | `androidMain/kotlin/com/augmentalis/llm/response/LLMContextBuilder.kt` | `estimateTokens()` uses the approximation `text.length / 4`. This is reasonable for English prose but significantly underestimates token count for code, CJK text (1 character ≈ 1 token, not 0.25), and whitespace-heavy content. Over-large prompts will silently exceed context limits. | Use the actual tokenizer's `encode()` method for accurate token counting, falling back to `length / 4` only if the tokenizer is unavailable. |
| **LOW** | `androidMain/kotlin/com/augmentalis/llm/alc/language/LanguagePackManager.kt:70` | `URL(manifestUrl).readText()` is a blocking network call executed on `Dispatchers.IO`. No timeout is set (defaults to OS timeout, which can be several minutes). The blocking `java.net.URL` API should not be used when an async HTTP client (OkHttp) is already a dependency. | Use `OkHttpClient` with explicit `connectTimeout` and `readTimeout` set to reasonable values (e.g., 15s). |
| **LOW** | `desktopMain/kotlin/com/augmentalis/llm/DesktopResponseGenerator.kt` | `String.format("%.2f", confidence)` is called in `buildPrompt()`. `String.format` is not available in KMP commonMain, but this file is desktopMain so it compiles. However, if the file is ever promoted to commonMain, it will break iOS/JS. | Use `"%.2f".format(confidence)` which is an extension available in Kotlin stdlib across targets, or `kotlin.math.round(confidence * 100) / 100.0`. |
| **LOW** | `androidMain/kotlin/com/augmentalis/llm/alc/tokenizer/SimpleVocabTokenizer.kt` | `SimpleVocabTokenizer` is described as "working tokenizer to unblock LocalLLMProvider while a full TVM tokenizer integration is developed." The class has hardcoded asset paths (`models/vocab.txt`, `models/gemma-2b-it/vocab.txt`) and word-level tokenization that is incompatible with BPE-based models. Shipping this as a real tokenizer produces garbage token sequences for Gemma/Llama. | Mark this class clearly as a test/debug-only stub, prevent instantiation in production builds via `check(BuildConfig.DEBUG)`, and wire `HuggingFaceTokenizer` or `TVMTokenizer` for all production inference paths. |
| **LOW** | `androidMain/kotlin/com/augmentalis/llm/alc/loader/HuggingFaceModelDownloader.kt` | No HTTPS certificate validation or pinning is configured on the `HttpURLConnection`. Downloads from HuggingFace are vulnerable to MITM if the network path is compromised (e.g., on corporate Wi-Fi with a custom CA). Combined with the missing checksums in `ChecksumHelper`, a MITM attacker can serve a poisoned model that the app will load and execute. | Either use `OkHttpClient` with certificate pinning for the HuggingFace CA, or ensure checksum verification is always performed post-download. |
| **LOW** | `commonMain/kotlin/com/augmentalis/llm/LLMProvider.kt` and `androidMain/kotlin/com/augmentalis/llm/response/ResponseGenerator.kt` | Two separate `ResponseGenerator` interfaces exist — one in `commonMain` (takes `intent: String, confidence: Float`) and one in `androidMain` (takes `IntentClassification`). Implementations in `androidMain` implement the androidMain version; `DesktopResponseGenerator` implements the commonMain version. This dual-interface design means commonMain code cannot use androidMain `ResponseGenerator` implementations, defeating the purpose of having a shared interface. | Unify: define a single `ResponseGenerator` in `commonMain` that takes the necessary parameters; move `IntentClassification` to commonMain if needed, or replace it with the primitive `intent: String, confidence: Float` pair everywhere. |
| **LOW** | `androidMain/kotlin/com/augmentalis/llm/alc/loader/ModelDownloader.kt:56` | Hardcoded download path `/sdcard/ava-ai-models/llm` uses the legacy `/sdcard/` symlink. On Android 10+ with scoped storage, apps cannot write to arbitrary external storage paths without `MANAGE_EXTERNAL_STORAGE`. | Replace with `context.getExternalFilesDir("ava-ai-models/llm")` for automatic permission compliance. |

---

## Recommendations

1. **Fix all three Rule 7 violations immediately** (`CommandInterpretation.kt`, `TemplateResponseGenerator.kt`, `LLMContextBuilder.kt`) and do a global search for "Claude" and "AI Team" in all file headers across the module. These are contractual violations.

2. **Treat the TAR path traversal (ALMExtractor.kt) as a security release blocker.** If users can load model archives from third-party sources, an attacker can craft a malicious archive and write arbitrary files to the device. Add both entry-name validation and canonical-path containment checks before writing any entry.

3. **Remove developer machine paths from ModelSelector.kt immediately.** Hardcoded `/Users/manoj_mbpm14/...` paths in a production class are a privacy violation and break builds on any other machine. Use `localSourcePath = null` in all production `ModelSpec` entries.

4. **Fix TVMModelLoader.first() to wait for download completion.** This is a logic correctness bug that makes the ALC engine non-functional for models that need downloading. Use `.first { it is DownloadState.Completed || it is DownloadState.Error }`.

5. **Implement the KV cache in BackpressureStreamingManager.** The current no-op extension stubs reduce inference performance dramatically (O(n²) instead of O(n) where n is context length). This is the core performance bottleneck for the on-device ALC engine.

6. **Unify the two ResponseGenerator interfaces.** Having `commonMain` and `androidMain` versions with incompatible signatures means the shared interface provides no contract guarantee. Pick one signature and apply it everywhere.

7. **Replace all `"TODO_GENERATE_AFTER_DOWNLOAD"` checksums** in `ChecksumHelper.kt` with real SHA-256 values before any model distribution goes live. Without integrity verification, the download pipeline is structurally unsound.

8. **Register `InferenceManager`'s BroadcastReceiver with `RECEIVER_NOT_EXPORTED`** to prevent the API 34+ crash on initialization.

9. **Fix `MultiProviderInferenceStrategy.init` sort discard** — `providers.sortedBy {...}` result is lost; fallback order is insertion order, not priority.

10. **`MLCInferenceStrategy.isAvailable()` should check native library presence** before returning `true`. A crash at JNI call time is much worse than being excluded from the provider list.

11. **Replace `GlobalScope.launch` in `LLMDownloadWorker`** with `coroutineScope { launch { ... } }` to keep notification updates tied to the worker's lifecycle.

12. **Address `ModelDownloader.listAvailableModels()` stub** — either implement it or remove it from the public API. A public method that unconditionally returns `emptyList()` is a Rule 1 violation.

13. **Move `ModelStorageManager` to `context.getExternalFilesDir()`** and update all `/sdcard/` hardcoded paths in `ModelDownloader` to use context-relative paths.

---

## File Health Summary

| Source Set | Files | Clean | Has Issues |
|------------|-------|-------|------------|
| commonMain | 6 | 4 | 2 (CommandInterpretation.kt rule7, ResponseGenerator dual-interface) |
| androidMain | ~75 | ~30 | ~45 |
| desktopMain | 4 | 3 | 1 (DesktopResponseGenerator minor) |
| Test files | 15 | 15 | 0 (tests reviewed, no issues found) |

**Total findings: 42** (9 CRITICAL / 14 HIGH / 13 MEDIUM / 16 LOW)
