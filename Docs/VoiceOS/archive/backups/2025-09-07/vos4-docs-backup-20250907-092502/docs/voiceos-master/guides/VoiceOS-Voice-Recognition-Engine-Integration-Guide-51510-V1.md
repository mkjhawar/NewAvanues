<!--
filename: Voice-Recognition-Engine-Integration-Guide.md
created: 2025-01-29 08:00:00 PST
author: VOS4 Development Team
purpose: Comprehensive guide for integrating new voice recognition engines
version: 2.0.0
location: /docs/guides/
target: Whisper engine integration research and standards documentation
-->

# Voice Recognition Engine Integration Guide

## üéØ Overview

This comprehensive guide provides research findings and implementation instructions for integrating new voice recognition engines into the VOS4 ecosystem, with special focus on Whisper.cpp integration. It analyzes the standard patterns from existing engines and provides detailed implementation guidance.

## üìã Research Summary

### Current Engine Architecture (4 Engines)

VOS4 currently implements 4 speech recognition engines following a standardized pattern:

| Engine | Type | Memory | Capabilities | Learning System |
|--------|------|--------|--------------|----------------|
| **AndroidSTTEngine** | Online | ~20MB | Native Android STT, wake/sleep, mode switching | ‚úÖ Enhanced multi-tier learning |
| **VoskEngine** | Offline | ~30MB | VOSK C++, dual recognizers, grammar constraints | ‚úÖ Four-tier caching system |
| **VivokaEngine** | Hybrid | ~60MB | VSDK, offline/online, wake word support | ‚úÖ Learning system integration |
| **GoogleCloudEngine** | Cloud | ~15MB | Premium features, 125+ languages, streaming | ‚úÖ Enhanced caching |

## üîç Whisper for Android Research Findings

### Implementation Options

#### 1. **Whisper.cpp (Recommended)**
- **Repository**: [ggml-org/whisper.cpp](https://github.com/ggml-org/whisper.cpp)
- **Performance**: Tiny/base models achieve "pretty good" performance on Android
- **Size**: Quantized models ~40MB (reasonable for VOS4 standards)
- **Speed**: ~2 seconds for 30 seconds of audio on Pixel-7
- **JNI Support**: Available via [whisper-jni](https://github.com/GiviMAD/whisper-jni)

#### 2. **TensorFlow Lite Implementation**
- **Repository**: [vilassn/whisper_android](https://github.com/vilassn/whisper_android)
- **Integration**: Java/Kotlin friendly TensorFlow Lite API
- **Performance**: Similar to whisper.cpp but with TF Lite overhead
- **Deployment**: Pre-built APKs available for testing

#### 3. **Recent Performance Improvements (2024-2025)**
- **Whisper Large V3 Turbo**: 5.4x speedup (216x RTFx)
- **Canary-1B-Flash**: 1000+ RTFx performance
- **Model Optimization**: Reduced decoder layers (32‚Üí4) maintaining accuracy

### Performance Benchmarks

```
Model Size vs Performance:
- Tiny Model: ~40MB, Fastest inference
- Base Model: ~75MB, Good accuracy/speed balance
- Small Model: ~245MB, Higher accuracy
- Medium/Large: >500MB, Not recommended for mobile

Speed Metrics:
- Real-time Factor: 216x (V3 Turbo)
- Inference Time: ~2s for 30s audio
- Memory Usage: 40-100MB depending on model
```

## üèóÔ∏è Standard Engine Integration Pattern

### Core Architecture Requirements

All engines follow this mandatory pattern:

```kotlin
/**
 * WhisperEngine - OpenAI Whisper speech recognition engine
 * 
 * MANDATORY: Uses all 4 shared components
 * MANDATORY: Implements learning system
 * MANDATORY: Supports mode switching and command matching
 */
class WhisperEngine(private val context: Context) {
    
    // ===== MANDATORY: Shared Components =====
    private val commandCache = CommandCache()
    private val timeoutManager = TimeoutManager(scope)
    private val resultProcessor = ResultProcessor()
    private val serviceState = ServiceState()
    
    // ===== Enhanced Learning System (MANDATORY) =====
    private val learnedCommands = ConcurrentHashMap<String, String>()
    private val vocabularyCache = ConcurrentHashMap<String, Boolean>()
    private val gson = Gson()
    
    // ===== Engine-Specific Components =====
    private var whisperClient: WhisperCppClient? = null
    private var config: SpeechConfig = SpeechConfig.default()
    
    // ===== Coroutine Management =====
    private val scope = CoroutineScope(
        SupervisorJob() + Dispatchers.IO + 
        CoroutineName("WhisperEngine")
    )
    
    // ===== State Management =====
    @Volatile private var isInitialized = false
    @Volatile private var isListening = false
    @Volatile private var isDictationActive = false
    @Volatile private var isVoiceEnabled = false
    @Volatile private var isVoiceSleeping = false
    @Volatile private var lastExecutedCommandTime = System.currentTimeMillis()
    
    // ===== Listeners =====
    private var resultListener: ((RecognitionResult) -> Unit)? = null
    private var errorListener: ((String, Int) -> Unit)? = null
    private var partialResultListener: ((String) -> Unit)? = null
}
```

### Required Methods (Mandatory Implementation)

```kotlin
// Core lifecycle methods
suspend fun initialize(config: SpeechConfig): Result<Unit>
fun startListening(): Result<Unit>
fun stopListening(): Result<Unit>
fun destroy(): Unit

// Command management (uses CommandCache)
fun setStaticCommands(commands: List<String>)
fun setContextPhrases(phrases: List<String>)

// Mode management
fun changeMode(mode: SpeechMode)
fun toggleMode()
fun isDictationMode(): Boolean

// Listener management
fun setResultListener(listener: (RecognitionResult) -> Unit)
fun setErrorListener(listener: (String, Int) -> Unit)
fun setPartialResultListener(listener: (String) -> Unit)

// Learning system (MANDATORY)
fun registerCommands(commands: List<String>)
fun getLearningStats(): Map<String, Int>
```

## üß† RecognitionLearning Integration Patterns

### Multi-Tier Learning System

All engines implement a sophisticated learning system with these components:

#### 1. **Learned Commands Cache**
```kotlin
// Persistent command learning
private val learnedCommands = ConcurrentHashMap<String, String>()

private suspend fun loadLearnedCommands() {
    withContext(Dispatchers.IO) {
        try {
            val file = File(context.filesDir, "WhisperLearnedCommands.json")
            if (file.exists()) {
                val json = file.readText()
                val type = object : TypeToken<Map<String, String>>() {}.type
                val loadedCommands: Map<String, String> = gson.fromJson(json, type) ?: emptyMap()
                learnedCommands.putAll(loadedCommands)
                Log.i(TAG, "üß† Whisper: Loaded ${learnedCommands.size} learned commands")
            }
        } catch (e: Exception) {
            Log.e(TAG, "üß† Whisper: Failed to load learned commands: ${e.message}")
        }
    }
}

private fun saveLearnedCommand(recognized: String, matched: String) {
    learnedCommands[recognized] = matched
    Log.d(TAG, "üß† Whisper Learning: '$recognized' ‚Üí '$matched'")
    
    scope.launch(Dispatchers.IO) {
        try {
            val file = File(context.filesDir, "WhisperLearnedCommands.json")
            file.writeText(gson.toJson(learnedCommands))
        } catch (e: Exception) {
            Log.e(TAG, "üß† Whisper: Failed to save learned commands: ${e.message}")
        }
    }
}
```

#### 2. **Vocabulary Cache**
```kotlin
private val vocabularyCache = ConcurrentHashMap<String, Boolean>()

private suspend fun loadVocabularyCache() {
    withContext(Dispatchers.IO) {
        try {
            val file = File(context.filesDir, "WhisperVocabularyCache.json")
            if (file.exists()) {
                val json = file.readText()
                val type = object : TypeToken<Map<String, Boolean>>() {}.type
                val loadedCache: Map<String, Boolean> = gson.fromJson(json, type) ?: emptyMap()
                vocabularyCache.putAll(loadedCache)
                Log.i(TAG, "üß† Whisper: Loaded ${vocabularyCache.size} vocabulary cache entries")
            }
        } catch (e: Exception) {
            Log.e(TAG, "üß† Whisper: Failed to load vocabulary cache: ${e.message}")
        }
    }
}
```

#### 3. **Enhanced Command Processing with Learning**
```kotlin
private suspend fun processCommandWithLearning(command: String): Pair<String?, Boolean> {
    var matchedCommand: String? = null
    var isSuccess = false
    
    // Tier 1: Check learned commands first (fastest)
    if (learnedCommands.containsKey(command)) {
        matchedCommand = learnedCommands[command]
        isSuccess = true
        Log.d(TAG, "üß† Whisper: Found learned command match: '$command' ‚Üí '$matchedCommand'")
    }
    // Tier 2: Use shared CommandCache for similarity matching
    else {
        val match = commandCache.findMatch(command)
        if (match != null) {
            matchedCommand = match
            isSuccess = true
            // Auto-learn successful similarity matches
            saveLearnedCommand(command, match)
            Log.d(TAG, "üß† Whisper: Found similarity match and learned: '$command' ‚Üí '$match'")
        }
    }
    
    return Pair(matchedCommand, isSuccess)
}
```

### ObjectBox Integration Pattern

For persistent learning data, follow the VosDataManager pattern:

#### 1. **Create Recognition Learning Entity**
```kotlin
// File: /managers/VosDataManager/src/main/java/com/augmentalis/vosdatamanager/entities/RecognitionLearningData.kt
@Entity
data class RecognitionLearningData(
    @Id var id: Long = 0,
    val engine: String = "", // "WHISPER", "VOSK", etc.
    val recognizedText: String = "",
    val matchedCommand: String = "",
    val confidence: Float = 0f,
    val userId: String? = null,
    val timestamp: Long = System.currentTimeMillis(),
    val learnedFromSimilarity: Boolean = false,
    val successfulMatch: Boolean = false,
    val contextMode: String = "", // "COMMAND", "DICTATION", etc.
    val metadata: String = "" // JSON for additional data
)
```

#### 2. **Create Learning Repository**
```kotlin
// File: /managers/VosDataManager/src/main/java/com/augmentalis/vosdatamanager/data/RecognitionLearning.kt
class RecognitionLearningRepo {
    
    val box: Box<RecognitionLearningData> = ObjectBox.store.boxFor(RecognitionLearningData::class.java)
    
    suspend fun insert(entity: RecognitionLearningData): Long = withContext(Dispatchers.IO) {
        box.put(entity)
    }
    
    suspend fun getLearnedCommands(engine: String): List<RecognitionLearningData> = withContext(Dispatchers.IO) {
        box.query()
            .equal(RecognitionLearningData_.engine, engine)
            .equal(RecognitionLearningData_.successfulMatch, true)
            .build()
            .find()
    }
    
    suspend fun updateLearningSuccess(
        engine: String,
        recognizedText: String,
        matchedCommand: String,
        confidence: Float,
        contextMode: String
    ) = withContext(Dispatchers.IO) {
        val entity = RecognitionLearningData(
            engine = engine,
            recognizedText = recognizedText,
            matchedCommand = matchedCommand,
            confidence = confidence,
            successfulMatch = true,
            contextMode = contextMode,
            learnedFromSimilarity = true
        )
        box.put(entity)
    }
    
    suspend fun getLearningStats(engine: String): Map<String, Int> = withContext(Dispatchers.IO) {
        val total = box.query().equal(RecognitionLearningData_.engine, engine).build().count()
        val successful = box.query()
            .equal(RecognitionLearningData_.engine, engine)
            .equal(RecognitionLearningData_.successfulMatch, true)
            .build().count()
        
        mapOf(
            "totalAttempts" to total.toInt(),
            "successfulMatches" to successful.toInt(),
            "learningRate" to if (total > 0) ((successful * 100) / total).toInt() else 0
        )
    }
}
```

## üîß Whisper Engine Implementation Guide

### Step 1: Add Whisper to Engine Enum

```kotlin
// File: /libraries/SpeechRecognition/src/main/java/com/augmentalis/speechrecognition/SpeechConfiguration.kt
enum class SpeechEngine {
    VOSK,
    VIVOKA, 
    ANDROID_STT,
    GOOGLE_CLOUD,
    WHISPER; // Add Whisper engine
    
    fun isOfflineCapable(): Boolean {
        return this in listOf(VOSK, VIVOKA, WHISPER)
    }
    
    fun getDisplayName(): String {
        return when (this) {
            // ... existing cases
            WHISPER -> "OpenAI Whisper"
        }
    }
}
```

### Step 2: Implement Whisper.cpp JNI Integration

```kotlin
// File: /libraries/SpeechRecognition/src/main/java/com/augmentalis/speechrecognition/speechengines/WhisperEngine.kt

/**
 * WhisperEngine.kt - OpenAI Whisper speech recognition engine
 * 
 * Features:
 * - Offline speech recognition using whisper.cpp
 * - Multiple model sizes (tiny, base, small)
 * - Enhanced learning system integration
 * - Mode switching (command/dictation)
 * - CommandCache integration
 * - RecognitionLearning ObjectBox integration
 */
class WhisperEngine(private val context: Context) {
    
    companion object {
        private const val TAG = "WhisperEngine"
        private const val DEFAULT_MODEL_SIZE = "tiny"
        private const val SAMPLE_RATE = 16000
        private const val CHANNELS = 1
        
        // Load whisper.cpp native library
        init {
            try {
                System.loadLibrary("whisper")
                Log.i(TAG, "Whisper native library loaded successfully")
            } catch (e: UnsatisfiedLinkError) {
                Log.e(TAG, "Failed to load Whisper native library", e)
            }
        }
    }
    
    // ===== MANDATORY: Shared Components =====
    private val commandCache = CommandCache()
    private val timeoutManager = TimeoutManager(scope)
    private val resultProcessor = ResultProcessor()
    private val serviceState = ServiceState()
    
    // ===== Enhanced Learning System =====
    private val learnedCommands = ConcurrentHashMap<String, String>()
    private val vocabularyCache = ConcurrentHashMap<String, Boolean>()
    private val gson = Gson()
    
    // ===== Whisper-Specific Components =====
    private var whisperContext: Long = 0L // Native whisper context handle
    private var modelPath: String = ""
    private var audioRecorder: AudioRecorder? = null
    
    // ===== Configuration =====
    private var config: SpeechConfig = SpeechConfig.default()
    private var currentMode = SpeechMode.DYNAMIC_COMMAND
    
    // ===== Coroutine Management =====
    private val scope = CoroutineScope(
        SupervisorJob() + Dispatchers.IO + 
        CoroutineName("WhisperEngine")
    )
    
    // ===== State Management =====
    @Volatile private var isInitialized = false
    @Volatile private var isListening = false
    @Volatile private var isDictationActive = false
    @Volatile private var isVoiceEnabled = false
    @Volatile private var isVoiceSleeping = false
    @Volatile private var lastExecutedCommandTime = System.currentTimeMillis()
    
    // ===== Native Methods (JNI) =====
    private external fun whisperInit(modelPath: String): Long
    private external fun whisperTranscribe(
        context: Long, 
        audioData: FloatArray, 
        sampleRate: Int,
        language: String
    ): String
    private external fun whisperDestroy(context: Long)
    
    /**
     * Initialize the Whisper engine
     */
    suspend fun initialize(config: SpeechConfig): Result<Unit> {
        return withContext(Dispatchers.IO) {
            try {
                Log.i(TAG, "Initializing WhisperEngine with ${config.engine}")
                
                this@WhisperEngine.config = config
                
                // Update state
                serviceState.updateState(
                    ServiceState.State.INITIALIZING,
                    "Loading Whisper model..."
                )
                
                // Initialize model
                initModel()
                
                // Initialize learning system
                Log.i(TAG, "üß† Whisper: Initializing learning system...")
                loadLearnedCommands()
                loadVocabularyCache()
                
                // Initialize audio recorder
                initAudioRecorder()
                
                serviceState.updateState(ServiceState.State.INITIALIZED)
                isInitialized = true
                
                Result.success(Unit)
                
            } catch (e: Exception) {
                Log.e(TAG, "Failed to initialize Whisper engine", e)
                serviceState.updateState(
                    ServiceState.State.ERROR,
                    "Initialization failed: ${e.message}"
                )
                Result.failure(e)
            }
        }
    }
    
    private fun initModel() {
        // Determine model path
        modelPath = config.modelPath ?: getDefaultModelPath()
        
        // Extract model if needed
        if (!File(modelPath).exists()) {
            extractModelFromAssets()
        }
        
        // Initialize whisper context
        whisperContext = whisperInit(modelPath)
        if (whisperContext == 0L) {
            throw RuntimeException("Failed to initialize Whisper context")
        }
        
        Log.i(TAG, "Whisper model initialized: $modelPath")
    }
    
    private fun getDefaultModelPath(): String {
        return "${context.filesDir}/whisper/ggml-${DEFAULT_MODEL_SIZE}.bin"
    }
    
    private fun extractModelFromAssets() {
        try {
            val assetPath = "whisper/ggml-${DEFAULT_MODEL_SIZE}.bin"
            val targetFile = File(getDefaultModelPath())
            targetFile.parentFile?.mkdirs()
            
            context.assets.open(assetPath).use { input ->
                targetFile.outputStream().use { output ->
                    input.copyTo(output)
                }
            }
            
            Log.i(TAG, "Extracted Whisper model to: ${targetFile.absolutePath}")
        } catch (e: Exception) {
            throw RuntimeException("Failed to extract Whisper model", e)
        }
    }
    
    private fun initAudioRecorder() {
        audioRecorder = AudioRecorder.Builder()
            .setSampleRate(SAMPLE_RATE)
            .setChannels(CHANNELS)
            .setAudioFormat(AudioFormat.PCM_FLOAT)
            .build()
    }
    
    /**
     * Start listening for speech
     */
    fun startListening(): Result<Unit> {
        return try {
            if (!isInitialized) {
                return Result.failure(IllegalStateException("Engine not initialized"))
            }
            
            if (isListening) {
                return Result.success(Unit) // Already listening
            }
            
            // Update state
            serviceState.updateState(ServiceState.State.LISTENING)
            isListening = true
            
            // Start audio recording
            startAudioCapture()
            
            // Start timeout if configured
            config.timeoutDuration.let { duration ->
                timeoutManager.startTimeout(duration) {
                    handleTimeout()
                }
            }
            
            Log.d(TAG, "Started listening")
            Result.success(Unit)
            
        } catch (e: Exception) {
            Log.e(TAG, "Failed to start listening", e)
            serviceState.updateState(ServiceState.State.ERROR, e.message)
            Result.failure(e)
        }
    }
    
    private fun startAudioCapture() {
        scope.launch {
            try {
                audioRecorder?.startRecording { audioData ->
                    // Convert to float array and process
                    processAudioData(audioData)
                }
            } catch (e: Exception) {
                Log.e(TAG, "Audio capture error", e)
                handleError("Audio capture failed: ${e.message}", -1)
            }
        }
    }
    
    private fun processAudioData(audioData: FloatArray) {
        scope.launch {
            try {
                // Call native whisper transcription
                val result = whisperTranscribe(
                    whisperContext,
                    audioData,
                    SAMPLE_RATE,
                    config.language
                )
                
                if (result.isNotBlank()) {
                    processRecognitionResult(result)
                }
                
            } catch (e: Exception) {
                Log.e(TAG, "Transcription error", e)
                handleError("Transcription failed: ${e.message}", -1)
            }
        }
    }
    
    /**
     * Process recognition result with enhanced learning
     */
    private fun processRecognitionResult(recognizedText: String) {
        scope.launch {
            try {
                val command = recognizedText.lowercase().trim()
                Log.d(TAG, "Processing result: $command")
                
                // Check for special commands first
                if (handleSpecialCommands(command)) {
                    return@launch
                }
                
                // Enhanced command processing with learning
                val (learnedMatch, wasLearned) = processCommandWithLearning(command)
                
                val foundCommand = when {
                    // Use learned command if found
                    learnedMatch != null -> {
                        learnedMatch
                    }
                    // Check CommandCache for exact matches
                    commandCache.hasCommand(command) -> {
                        val match = commandCache.findMatch(command)
                        if (match != null && match != command) {
                            // Auto-learn successful CommandCache matches
                            saveLearnedCommand(command, match)
                            Log.d(TAG, "üß† Whisper: CommandCache match learned: '$command' ‚Üí '$match'")
                        }
                        match ?: command
                    }
                    else -> {
                        // Similarity matching fallback
                        val similarCommand = findMostSimilarCommand(command)
                        if (similarCommand != null && similarCommand != command) {
                            saveLearnedCommand(command, similarCommand)
                            Log.d(TAG, "üß† Whisper: Similarity match learned: '$command' ‚Üí '$similarCommand'")
                        }
                        similarCommand ?: command
                    }
                }
                
                // Log learning activity
                if (wasLearned && learnedMatch != null) {
                    Log.i(TAG, "üß† Whisper: Enhanced command via learning: '$command' ‚Üí '$learnedMatch'")
                }
                
                // Create recognition result
                val result = RecognitionResult(
                    text = foundCommand,
                    originalText = recognizedText,
                    confidence = if (foundCommand == command) 0.95f else 0.85f,
                    isFinal = true,
                    alternatives = emptyList(),
                    engine = "WHISPER",
                    mode = currentMode.name
                )
                
                // Notify listener
                withContext(Dispatchers.Main) {
                    resultListener?.invoke(result)
                }
                
                // Reset timeout
                timeoutManager.resetTimeout(config.timeoutDuration) {
                    handleTimeout()
                }
                
                lastExecutedCommandTime = System.currentTimeMillis()
                
            } catch (e: Exception) {
                Log.e(TAG, "Result processing error", e)
                handleError("Processing failed: ${e.message}", -1)
            }
        }
    }
    
    private fun handleSpecialCommands(command: String): Boolean {
        return when {
            command.equals(config.muteCommand, ignoreCase = true) -> {
                handleMuteCommand()
                true
            }
            command.equals(config.unmuteCommand, ignoreCase = true) -> {
                handleUnmuteCommand()
                true
            }
            command.equals(config.startDictationCommand, ignoreCase = true) -> {
                changeMode(SpeechMode.DICTATION)
                true
            }
            command.equals(config.stopDictationCommand, ignoreCase = true) -> {
                changeMode(SpeechMode.DYNAMIC_COMMAND)
                true
            }
            else -> false
        }
    }
    
    /**
     * Stop listening for speech
     */
    fun stopListening(): Result<Unit> {
        return try {
            if (!isListening) {
                return Result.success(Unit)
            }
            
            // Stop audio recording
            audioRecorder?.stopRecording()
            
            // Cancel timeout
            timeoutManager.cancelTimeout()
            
            // Update state
            isListening = false
            serviceState.updateState(ServiceState.State.READY)
            
            Log.d(TAG, "Stopped listening")
            Result.success(Unit)
            
        } catch (e: Exception) {
            Log.e(TAG, "Failed to stop listening", e)
            Result.failure(e)
        }
    }
    
    /**
     * Change speech recognition mode
     */
    fun changeMode(mode: SpeechMode) {
        if (currentMode == mode) return
        
        Log.i(TAG, "Changing mode to: $mode")
        
        scope.launch {
            try {
                val wasListening = isListening
                if (wasListening) {
                    stopListening()
                }
                
                when (mode) {
                    SpeechMode.DICTATION, SpeechMode.FREE_SPEECH -> {
                        isDictationActive = true
                        serviceState.updateState(ServiceState.State.FREE_SPEECH)
                    }
                    SpeechMode.DYNAMIC_COMMAND, SpeechMode.STATIC_COMMAND -> {
                        isDictationActive = false
                        serviceState.updateState(ServiceState.State.READY)
                    }
                    else -> Log.w(TAG, "Unsupported mode: $mode")
                }
                
                currentMode = mode
                
                if (wasListening) {
                    startListening()
                }
                
            } catch (e: Exception) {
                Log.e(TAG, "Failed to change mode to $mode", e)
            }
        }
    }
    
    /**
     * Set static commands for CommandCache
     */
    fun setStaticCommands(commands: List<String>) {
        Log.i(TAG, "Setting ${commands.size} static commands")
        commandCache.setStaticCommands(commands.filter { it.isNotBlank() })
    }
    
    /**
     * Set context phrases (dynamic commands)
     */
    fun setContextPhrases(phrases: List<String>) {
        Log.i(TAG, "Setting ${phrases.size} context phrases")
        
        if (phrases.isEmpty()) {
            Log.w(TAG, "Empty phrases list received")
            return
        }
        
        commandCache.setDynamicCommands(phrases.filter { it.isNotBlank() })
    }
    
    /**
     * Register commands for learning system
     */
    fun registerCommands(commands: List<String>) {
        scope.launch {
            commandCache.setStaticCommands(commands)
            Log.d(TAG, "üß† Whisper: Registered ${commands.size} commands for learning system")
        }
    }
    
    /**
     * Get learning statistics
     */
    fun getLearningStats(): Map<String, Int> {
        return mapOf(
            "learnedCommands" to learnedCommands.size,
            "vocabularyCache" to vocabularyCache.size
        )
    }
    
    // ===== Learning System Implementation =====
    
    private suspend fun loadLearnedCommands() {
        withContext(Dispatchers.IO) {
            try {
                val file = File(context.filesDir, "WhisperLearnedCommands.json")
                if (file.exists()) {
                    val json = file.readText()
                    val type = object : TypeToken<Map<String, String>>() {}.type
                    val loadedCommands: Map<String, String> = gson.fromJson(json, type) ?: emptyMap()
                    learnedCommands.putAll(loadedCommands)
                    Log.i(TAG, "üß† Whisper: Loaded ${learnedCommands.size} learned commands")
                }
            } catch (e: Exception) {
                Log.e(TAG, "üß† Whisper: Failed to load learned commands: ${e.message}")
            }
        }
    }
    
    private suspend fun loadVocabularyCache() {
        withContext(Dispatchers.IO) {
            try {
                val file = File(context.filesDir, "WhisperVocabularyCache.json")
                if (file.exists()) {
                    val json = file.readText()
                    val type = object : TypeToken<Map<String, Boolean>>() {}.type
                    val loadedCache: Map<String, Boolean> = gson.fromJson(json, type) ?: emptyMap()
                    vocabularyCache.putAll(loadedCache)
                    Log.i(TAG, "üß† Whisper: Loaded ${vocabularyCache.size} vocabulary cache entries")
                }
            } catch (e: Exception) {
                Log.e(TAG, "üß† Whisper: Failed to load vocabulary cache: ${e.message}")
            }
        }
    }
    
    private fun saveLearnedCommand(recognized: String, matched: String) {
        learnedCommands[recognized] = matched
        Log.d(TAG, "üß† Whisper Learning: '$recognized' ‚Üí '$matched'")
        
        scope.launch(Dispatchers.IO) {
            try {
                val file = File(context.filesDir, "WhisperLearnedCommands.json")
                file.writeText(gson.toJson(learnedCommands))
            } catch (e: Exception) {
                Log.e(TAG, "üß† Whisper: Failed to save learned commands: ${e.message}")
            }
        }
    }
    
    private suspend fun processCommandWithLearning(command: String): Pair<String?, Boolean> {
        var matchedCommand: String? = null
        var isSuccess = false
        
        // Tier 1: Check learned commands first (fastest)
        if (learnedCommands.containsKey(command)) {
            matchedCommand = learnedCommands[command]
            isSuccess = true
            Log.d(TAG, "üß† Whisper: Found learned command match: '$command' ‚Üí '$matchedCommand'")
        }
        // Tier 2: Use shared CommandCache for similarity matching
        else {
            val match = commandCache.findMatch(command)
            if (match != null) {
                matchedCommand = match
                isSuccess = true
                // Auto-learn successful similarity matches
                saveLearnedCommand(command, match)
                Log.d(TAG, "üß† Whisper: Found similarity match and learned: '$command' ‚Üí '$match'")
            }
        }
        
        return Pair(matchedCommand, isSuccess)
    }
    
    private fun findMostSimilarCommand(text: String): String? {
        val normalizedText = text.lowercase().trim()
        val allCommands = commandCache.getAllCommands()
        
        var bestMatch: String? = null
        var bestSimilarity = 0.0f
        
        for (command in allCommands) {
            val similarity = calculateSimilarity(normalizedText, command)
            if (similarity > bestSimilarity && similarity > 0.6f) {
                bestSimilarity = similarity
                bestMatch = command
            }
        }
        
        return bestMatch
    }
    
    private fun calculateSimilarity(s1: String, s2: String): Float {
        val distance = levenshteinDistance(s1, s2)
        val maxLength = maxOf(s1.length, s2.length)
        return if (maxLength == 0) 1.0f else 1.0f - (distance.toFloat() / maxLength)
    }
    
    private fun levenshteinDistance(s1: String, s2: String): Int {
        val len1 = s1.length
        val len2 = s2.length
        
        val dp = Array(len1 + 1) { IntArray(len2 + 1) }
        
        for (i in 0..len1) dp[i][0] = i
        for (j in 0..len2) dp[0][j] = j
        
        for (i in 1..len1) {
            for (j in 1..len2) {
                val cost = if (s1[i - 1] == s2[j - 1]) 0 else 1
                dp[i][j] = minOf(
                    dp[i - 1][j] + 1,
                    dp[i][j - 1] + 1,
                    dp[i - 1][j - 1] + cost
                )
            }
        }
        
        return dp[len1][len2]
    }
    
    // ===== State Management =====
    
    private fun handleMuteCommand() {
        isVoiceSleeping = true
        serviceState.updateState(ServiceState.State.SLEEPING)
        Log.d(TAG, "Voice muted")
    }
    
    private fun handleUnmuteCommand() {
        isVoiceSleeping = false
        serviceState.updateState(ServiceState.State.READY)
        lastExecutedCommandTime = System.currentTimeMillis()
        Log.d(TAG, "Voice unmuted")
    }
    
    private fun handleTimeout() {
        Log.d(TAG, "Recognition timeout")
        stopListening()
        
        scope.launch(Dispatchers.Main) {
            errorListener?.invoke("Recognition timeout", -1)
        }
    }
    
    private fun handleError(message: String, code: Int) {
        serviceState.updateState(ServiceState.State.ERROR, message)
        
        scope.launch(Dispatchers.Main) {
            errorListener?.invoke(message, code)
        }
    }
    
    // ===== Listener Management =====
    
    fun setResultListener(listener: (RecognitionResult) -> Unit) {
        this.resultListener = listener
    }
    
    fun setErrorListener(listener: (String, Int) -> Unit) {
        this.errorListener = listener
    }
    
    fun setPartialResultListener(listener: (String) -> Unit) {
        this.partialResultListener = listener
    }
    
    fun toggleMode() {
        val newMode = when (currentMode) {
            SpeechMode.FREE_SPEECH, SpeechMode.DICTATION -> SpeechMode.DYNAMIC_COMMAND
            SpeechMode.DYNAMIC_COMMAND -> SpeechMode.DICTATION
            else -> SpeechMode.DYNAMIC_COMMAND
        }
        changeMode(newMode)
    }
    
    fun isDictationMode(): Boolean = isDictationActive
    
    /**
     * Destroy the engine and cleanup resources
     */
    fun destroy() {
        Log.d(TAG, "Destroying WhisperEngine")
        
        try {
            // Stop listening
            stopListening()
            
            // Destroy audio recorder
            audioRecorder?.release()
            
            // Destroy whisper context
            if (whisperContext != 0L) {
                whisperDestroy(whisperContext)
                whisperContext = 0L
            }
            
            // Cancel all coroutines
            scope.cancel()
            
            // Clear caches
            commandCache.clear()
            learnedCommands.clear()
            vocabularyCache.clear()
            
            // Reset state
            isInitialized = false
            serviceState.updateState(ServiceState.State.SHUTDOWN)
            
            Log.d(TAG, "WhisperEngine destroyed")
            
        } catch (e: Exception) {
            Log.e(TAG, "Destroy error", e)
        }
    }
}
```

## üìä COT/TOT Analysis for Whisper Integration

### Chain of Thought (COT) Analysis

**Problem**: Integrating OpenAI Whisper as a 5th engine in VOS4's speech recognition system.

**Approach**:
1. **Research Phase**: Analyzed whisper.cpp implementations, found JNI solutions
2. **Pattern Analysis**: Studied 4 existing engines, identified common architecture
3. **Learning Integration**: Examined how learning systems work across engines
4. **ObjectBox Integration**: Analyzed VosDataManager patterns for persistence
5. **Implementation Planning**: Created step-by-step integration guide

**Key Insights**:
- All engines use 4 mandatory shared components (CommandCache, TimeoutManager, ResultProcessor, ServiceState)
- Learning systems are consistently implemented with JSON persistence + in-memory caching
- Mode switching (command/dictation) is standardized across engines
- ObjectBox integration follows established VosDataManager patterns

### Train of Thought (TOT) Analysis - Alternative Approaches

#### Option 1: Direct whisper.cpp Integration (Recommended)
**Pros**: 
- Lightweight (~40MB models)
- Fast inference (2s for 30s audio)
- Offline capability
- Active community support

**Cons**: 
- JNI complexity
- Native library management
- Model extraction required

**Decision**: Recommended due to performance and offline capability alignment with VOS4 goals.

#### Option 2: TensorFlow Lite Integration
**Pros**: 
- Java/Kotlin friendly
- Well-documented Android integration
- Google ecosystem support

**Cons**: 
- TF Lite overhead
- Larger memory footprint
- More complex dependency management

**Decision**: Alternative option if JNI proves problematic.

#### Option 3: Cloud-based Whisper
**Pros**: 
- Latest models available
- No local storage requirements
- Higher accuracy potential

**Cons**: 
- Network dependency
- Latency concerns
- Privacy implications
- Against VOS4 offline-first philosophy

**Decision**: Not recommended for primary implementation.

### Reflection on Best Approach

**Optimal Solution**: whisper.cpp with JNI integration

**Reasoning**:
1. **Alignment**: Matches VOS4's offline-first architecture
2. **Performance**: Meets mobile performance requirements
3. **Consistency**: Follows established 4-engine pattern perfectly
4. **Learning**: Can leverage existing learning system architecture
5. **Memory**: Fits within VOS4's memory budget constraints

**Implementation Priority**:
1. Start with tiny model for proof of concept
2. Implement core functionality following existing patterns
3. Add learning system integration
4. Optimize performance and memory usage
5. Add larger models as optional features

## üß™ Testing Requirements

### Unit Tests

```kotlin
class WhisperEngineTest {
    
    @Test
    fun testInitialization() = runTest {
        val engine = WhisperEngine(context)
        val config = SpeechConfig().withEngine(SpeechEngine.WHISPER)
        
        val result = engine.initialize(config)
        assertTrue(result.isSuccess)
    }
    
    @Test
    fun testLearningSystem() = runTest {
        val engine = WhisperEngine(context)
        engine.setStaticCommands(listOf("test command"))
        
        // Simulate recognition and learning
        val stats = engine.getLearningStats()
        assertTrue(stats.containsKey("learnedCommands"))
    }
    
    @Test
    fun testModeSwitching() = runTest {
        val engine = WhisperEngine(context)
        engine.initialize(SpeechConfig())
        
        engine.changeMode(SpeechMode.DICTATION)
        assertTrue(engine.isDictationMode())
        
        engine.changeMode(SpeechMode.DYNAMIC_COMMAND)
        assertFalse(engine.isDictationMode())
    }
}
```

### Performance Tests

```kotlin
class WhisperPerformanceTest {
    
    @Test
    fun testInferenceSpeed() = runTest {
        val engine = WhisperEngine(context)
        engine.initialize(SpeechConfig())
        
        val startTime = System.currentTimeMillis()
        // Simulate 30 seconds of audio processing
        val endTime = System.currentTimeMillis()
        
        val processingTime = endTime - startTime
        assertTrue("Processing time should be < 3000ms", processingTime < 3000)
    }
    
    @Test
    fun testMemoryUsage() {
        val engine = WhisperEngine(context)
        val memoryBefore = getMemoryUsage()
        
        engine.initialize(SpeechConfig())
        val memoryAfter = getMemoryUsage()
        
        val memoryDiff = memoryAfter - memoryBefore
        assertTrue("Memory usage should be < 60MB", memoryDiff < 60 * 1024 * 1024)
    }
}
```

## üìã Integration Checklist

### Pre-Integration
- [ ] Set up whisper.cpp native library
- [ ] Create JNI bindings for Android
- [ ] Extract/download required models
- [ ] Test basic transcription functionality

### Core Implementation
- [ ] Add WHISPER to SpeechEngine enum
- [ ] Implement WhisperEngine class with all required methods
- [ ] Integrate 4 shared components (CommandCache, TimeoutManager, etc.)
- [ ] Implement learning system with JSON persistence
- [ ] Add mode switching support (command/dictation)
- [ ] Implement error handling and state management

### Learning System Integration
- [ ] Create RecognitionLearningData ObjectBox entity
- [ ] Implement RecognitionLearningRepo with VosDataManager
- [ ] Integrate multi-tier command matching
- [ ] Add vocabulary caching system
- [ ] Implement learning statistics tracking

### Testing & Validation
- [ ] Unit tests for core functionality
- [ ] Performance benchmarking
- [ ] Memory usage validation
- [ ] Learning system testing
- [ ] Integration with existing VOS4 components

### Documentation & Deployment
- [ ] Update feature matrix documentation
- [ ] Create Whisper-specific configuration guide
- [ ] Update API documentation
- [ ] Add performance metrics documentation
- [ ] Integration with VoiceRecognition app

## üìà Performance Benchmarks

### Target Metrics for Whisper Engine

| Metric | Target | Rationale |
|--------|---------|-----------|
| **Initialization Time** | < 3 seconds | Model loading time |
| **Inference Speed** | < 2 seconds for 30s audio | Real-time performance |
| **Memory Usage** | < 60MB | Comparable to Vivoka |
| **Recognition Accuracy** | > 90% for commands | Command matching effectiveness |
| **Learning Rate** | > 80% successful matches | Learning system efficiency |
| **Battery Impact** | < 5% per hour | Mobile device efficiency |

### Model Size vs Performance Trade-offs

```
Whisper Model Comparison:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model   ‚îÇ Size    ‚îÇ Accuracy    ‚îÇ Speed        ‚îÇ Recommended ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Tiny    ‚îÇ ~40MB   ‚îÇ Good        ‚îÇ Fastest      ‚îÇ ‚úÖ Default  ‚îÇ
‚îÇ Base    ‚îÇ ~75MB   ‚îÇ Better      ‚îÇ Fast         ‚îÇ ‚úÖ Option   ‚îÇ
‚îÇ Small   ‚îÇ ~245MB  ‚îÇ Very Good   ‚îÇ Medium       ‚îÇ ‚ö†Ô∏è  Premium ‚îÇ
‚îÇ Medium  ‚îÇ ~775MB  ‚îÇ Excellent   ‚îÇ Slow         ‚îÇ ‚ùå Too big  ‚îÇ
‚îÇ Large   ‚îÇ ~1.5GB  ‚îÇ Best        ‚îÇ Very Slow    ‚îÇ ‚ùå Too big  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Implementation Roadmap

### Phase 1: Foundation (Week 1-2)
1. Set up whisper.cpp build environment
2. Create basic JNI bindings
3. Implement core WhisperEngine structure
4. Basic audio capture and transcription

### Phase 2: Integration (Week 3-4)
1. Integrate 4 shared components
2. Implement learning system
3. Add mode switching functionality
4. Basic error handling and state management

### Phase 3: Enhancement (Week 5-6)
1. ObjectBox RecognitionLearning integration
2. Performance optimization
3. Memory usage optimization
4. Advanced error handling

### Phase 4: Testing & Polish (Week 7-8)
1. Comprehensive testing suite
2. Performance benchmarking
3. Documentation completion
4. Integration with VoiceRecognition app

## üîç Best Practices

### Memory Management
- Use tiny model by default, provide base as option
- Implement proper native memory cleanup
- Monitor and limit audio buffer sizes
- Clear caches periodically

### Performance Optimization
- Pre-load models during initialization
- Use background threads for processing
- Implement audio streaming for real-time processing
- Cache frequently used results

### Error Handling
- Graceful fallback if native library fails
- Network-based fallback for critical errors
- User-friendly error messages
- Automatic recovery mechanisms

### Learning System
- Balance learning speed vs accuracy
- Implement learning confidence thresholds
- Regular cleanup of outdated learning data
- Privacy-conscious learning (no personal data)

---

**Final Notes**: This guide provides a comprehensive roadmap for integrating Whisper as the 5th speech recognition engine in VOS4. The implementation follows established patterns while leveraging Whisper's unique capabilities for offline, high-quality speech recognition. The learning system integration ensures that Whisper benefits from the same adaptive capabilities as other engines, making it a powerful addition to the VOS4 ecosystem.