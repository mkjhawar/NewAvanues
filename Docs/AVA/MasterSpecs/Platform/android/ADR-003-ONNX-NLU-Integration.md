# ADR-003: ONNX Runtime for Android NLU

**Status**: Accepted
**Date**: 2025-10-29
**Authors**: Manoj Jhawar
**Deciders**: Architecture Team
**Platform**: Android
**Related**: ADR-001 (KMP Strategy)

---

## Context

AVA AI needs on-device Natural Language Understanding (NLU) for Android that:
1. Runs 100% locally (privacy-first, no cloud API calls)
2. Achieves <100ms inference latency (target: <50ms)
3. Supports intent classification + entity extraction
4. Works on low-end devices (<512MB available RAM)
5. Model size <50MB (app size constraint)
6. Integrates with Teach-Ava training system

---

## Decision

**We will use ONNX Runtime Mobile 1.17.0 with MobileBERT INT8 for Android NLU:**

### Model
- **Architecture**: MobileBERT (Google)
- **Quantization**: INT8 (from FP32)
- **Size**: 25.5 MB (ONNX format)
- **Vocab**: 30,522 tokens (WordPiece tokenizer)
- **Source**: `onnx-community/mobilebert-uncased-ONNX`

### Runtime
- **Library**: ONNX Runtime Mobile 1.17.0
- **Backend**: CPU (NNAPI acceleration where available)
- **Location**: `platform/app/src/main/assets/models/`

### Components
```kotlin
// features/nlu/src/androidMain/kotlin/
â”œâ”€â”€ BertTokenizer.kt           # WordPiece tokenization (30,522 vocab)
â”œâ”€â”€ IntentClassifier.kt        # ONNX inference pipeline
â”œâ”€â”€ ModelManager.kt            # Model loading + lifecycle
â””â”€â”€ ClassifyIntentUseCase.kt   # Business logic wrapper
```

---

## Rationale

### Why ONNX Runtime?

**Performance**: Fastest mobile inference runtime (2x faster than TensorFlow Lite for BERT models)

**Model Support**: Supports BERT, DistilBERT, MobileBERT, ALBERT, RoBERTa (all transformer architectures)

**Optimization**: Built-in INT8 quantization, graph optimizations, NNAPI acceleration

**Industry Adoption**: Used by Microsoft (Office, Edge), LinkedIn, Uber, Snapchat

**Proven**: AVA AI already implemented (36 tests, 92% coverage, <50ms target inference)

### Why MobileBERT?

**Size**: 25.5 MB (vs 110 MB BERT-base, 440 MB BERT-large)

**Speed**: 4x faster than BERT-base (optimized for mobile)

**Accuracy**: 99.2% of BERT-base accuracy (minimal quality loss)

**Quantization**: INT8 quantization reduces size 4x with <1% accuracy drop

**Mobile-First**: Designed by Google specifically for on-device inference

### Why NOT Alternatives?

| Alternative | Rejected Because |
|-------------|-----------------|
| **TensorFlow Lite** | 2x slower than ONNX for BERT, larger runtime (5MB vs 2MB) |
| **MediaPipe** | No BERT support, focused on vision/audio tasks |
| **ML Kit** | Cloud-based, violates privacy-first principle |
| **PyTorch Mobile** | 10MB runtime overhead, slower than ONNX |
| **Custom LSTM** | Poor accuracy vs BERT (<80% intent classification) |

---

## Architecture

### Inference Pipeline

```kotlin
// 1. User input
val userInput = "Set a timer for 5 minutes"

// 2. Tokenization (BertTokenizer)
val tokens = tokenizer.tokenize(userInput)
// â†’ ["[CLS]", "set", "a", "timer", "for", "5", "minutes", "[SEP]"]

val inputIds = tokenizer.convertToIds(tokens)
// â†’ [101, 2275, 1037, 8346, 2005, 1019, 2781, 102]

// 3. ONNX Inference (IntentClassifier)
val logits = classifier.classify(inputIds)
// â†’ FloatArray[10] (one score per intent)

// 4. Softmax + Argmax
val intentIndex = logits.argMax()
val confidence = softmax(logits)[intentIndex]
// â†’ intent: "timer.set", confidence: 0.94

// 5. Teach-Ava Fallback (if confidence < 0.7)
if (confidence < 0.7) {
    suggestTeachAva(userInput)
}
```

### Model Loading

```kotlin
// ModelManager.kt
class ModelManager(context: Context) {
    private val env = OrtEnvironment.getEnvironment()
    private var session: OrtSession? = null

    fun loadModel() {
        val modelBytes = context.assets.open("models/mobilebert_int8.onnx")
            .readBytes()

        session = env.createSession(modelBytes,
            OrtSession.SessionOptions().apply {
                setIntraOpNumThreads(2)  // Use 2 CPU cores
                setExecutionMode(ExecutionMode.SEQUENTIAL)
            })
    }
}
```

### Performance Budgets

| Stage | Budget | Actual | Status |
|-------|--------|--------|--------|
| Model loading | <500ms | ~300ms | âœ… Good |
| Tokenization | <5ms | TBD | â³ Device test |
| ONNX inference | <50ms target, <100ms max | TBD | â³ Device test |
| **Total (cold start)** | <600ms | TBD | â³ Device test |
| **Total (warm)** | <60ms | TBD | â³ Device test |

---

## Consequences

### Positive

âœ… **100% local** â†’ Zero cloud API calls, zero network latency, zero API costs

âœ… **Privacy-first** â†’ User data never leaves device

âœ… **Fast** â†’ <100ms inference (target: <50ms)

âœ… **Small** â†’ 25.5MB model + 2MB runtime = 27.5MB total

âœ… **Accurate** â†’ 99.2% BERT-base quality

âœ… **Teach-Ava ready** â†’ Low confidence triggers training UI

âœ… **Proven** â†’ 36 tests, 92% coverage

### Negative

âš ï¸ **Android-only** â†’ iOS needs separate Core ML implementation (expect/actual pattern)

âš ï¸ **Device validation needed** â†’ Performance budgets not tested on real devices yet (Week 6)

âš ï¸ **Model updates** â†’ Requires app update (no over-the-air model updates)

âš ï¸ **Training offline** â†’ No cloud training pipeline (Teach-Ava stores examples locally)

### Neutral

ðŸ”„ **NNAPI acceleration** â†’ Automatic on supported devices (Pixel 3+, Samsung S9+)

ðŸ”„ **Memory usage** â†’ 27.5MB model + 50MB inference RAM = 77.5MB total (within 512MB budget)

---

## Implementation Status

### Complete (Week 5) âœ…
- âœ… ONNX Runtime Mobile 1.17.0 integration
- âœ… MobileBERT INT8 model loaded (25.5 MB)
- âœ… BertTokenizer implementation (30,522 vocab)
- âœ… IntentClassifier pipeline
- âœ… ModelManager lifecycle
- âœ… 36 tests (92% coverage)
- âœ… Teach-Ava UI integration

### Pending (Week 6) â³
- â³ Device testing (validate <100ms budget)
- â³ Low-end device testing (RAM usage)
- â³ Battery impact measurement
- â³ NNAPI acceleration validation

### Future (Phase 2) ðŸ”®
- ðŸ”® iOS Core ML implementation (expect/actual)
- ðŸ”® Model fine-tuning with Teach-Ava data
- ðŸ”® Entity extraction (currently intent-only)
- ðŸ”® Multi-language support (currently English)

---

## iOS Strategy (Parallel Implementation)

**For iOS, we will use Core ML (NOT ONNX):**

```kotlin
// features/nlu/src/commonMain/kotlin/
expect class PlatformIntentClassifier {
    suspend fun classify(text: String): IntentResult
}

// features/nlu/src/androidMain/kotlin/
actual class PlatformIntentClassifier {
    // ONNX implementation (this ADR)
}

// features/nlu/src/iosMain/kotlin/
actual class PlatformIntentClassifier {
    // Core ML implementation (future ADR)
}
```

**Why Core ML for iOS?**
- Native Apple framework (optimized for ANE/GPU)
- Zero extra dependencies (built into iOS)
- Better battery efficiency than ONNX on iOS
- Easier App Store approval (no third-party ML runtimes)

**Model Conversion**: MobileBERT ONNX â†’ Core ML (using `coremltools`)

---

## Testing Strategy

### Unit Tests (36 tests, 92% coverage)
```kotlin
// ModelLoadingTest.kt
@Test fun modelLoadsSuccessfully()
@Test fun modelHandlesInvalidAssetPath()

// BertTokenizerTest.kt
@Test fun tokenizationHandlesEmptyInput()
@Test fun tokenizationHandlesSpecialCharacters()
@Test fun vocabularyLoadsCorrectly()

// IntentClassifierTest.kt
@Test fun classificationReturnsValidIntent()
@Test fun lowConfidenceTriggersTeachAva()
@Test fun inferenceTimeWithinBudget()  // Currently mock, needs device test
```

### Device Tests (Week 6)
```kotlin
// PerformanceInstrumentedTest.kt
@Test fun coldStartInferenceUnder600ms()
@Test fun warmInferenceUnder60ms()
@Test fun memoryUsageUnder100MB()
@Test fun batteryImpactUnder10PercentPerHour()

// LowEndDeviceTest.kt
@Test fun worksOn512MBRamDevice()
@Test fun gracefulDegradationOn1CoreCPU()
```

---

## Alternatives Considered (Detailed)

### 1. TensorFlow Lite
**Pros**: Google-backed, mature ecosystem, TFLite Model Maker
**Cons**: 2x slower than ONNX for BERT, 5MB runtime (vs 2MB ONNX), less optimized for transformers
**Verdict**: ONNX faster and smaller

### 2. ML Kit (Cloud)
**Pros**: High accuracy, no on-device model, Google-backed
**Cons**: Violates privacy-first principle, requires network, API costs, latency varies
**Verdict**: Incompatible with AVA's core values

### 3. Custom LSTM (Trained from scratch)
**Pros**: Smaller model (<5MB), faster inference (<10ms)
**Cons**: Poor accuracy (<80% intent classification), requires large training dataset (10K+ examples), maintenance burden
**Verdict**: Accuracy too low, BERT pre-trained models better

### 4. DistilBERT (Smaller BERT variant)
**Pros**: 40% smaller than BERT-base (66MB), faster than BERT-base
**Cons**: Still larger than MobileBERT (66MB vs 25.5MB), not optimized for mobile
**Verdict**: MobileBERT better for mobile constraints

### 5. ALBERT (Shared-Parameter BERT)
**Pros**: Small model (12MB), similar accuracy to BERT
**Cons**: Slower inference (parameter sharing = more compute), less tested in production
**Verdict**: MobileBERT faster and more proven

---

## Future Enhancements

### Phase 2: Entity Extraction
- Add named entity recognition (NER) layer
- Extract parameters from user input ("5 minutes", "tomorrow 3pm")
- Requires additional ONNX model or custom head

### Phase 3: Fine-Tuning
- Collect Teach-Ava examples (user training data)
- Fine-tune MobileBERT on AVA-specific intents
- Improve accuracy for domain-specific language

### Phase 4: Multi-Language
- Add multilingual models (mBERT, XLM-RoBERTa)
- Support Spanish, French, German, Chinese
- Trade-off: Larger model (50MB â†’ 100MB)

---

## References

- [ONNX Runtime Mobile](https://onnxruntime.ai/docs/tutorials/mobile/)
- [MobileBERT Paper](https://arxiv.org/abs/2004.02984)
- [ONNX vs TFLite Benchmark](https://onnxruntime.ai/docs/performance/benchmarks.html)
- [Hugging Face ONNX Models](https://huggingface.co/onnx-community/mobilebert-uncased-ONNX)
- AVA AI NLU Test Suite (`features/nlu/src/androidTest/`)

---

## Changelog

**v1.0 (2025-10-29)**: Initial decision - ONNX Runtime Mobile + MobileBERT INT8 for Android NLU

---

**Created by Manoj Jhawar, manoj@ideahq.net**
