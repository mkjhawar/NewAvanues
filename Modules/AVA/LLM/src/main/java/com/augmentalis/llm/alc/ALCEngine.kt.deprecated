/**
 * ALC Engine (Adaptive LLM Coordinator Engine)
 *
 * Core inference engine for on-device LLM in AVA AI.
 * Complete rewrite tailored for AVA's privacy-first, offline-capable architecture.
 *
 * Key Features:
 * - Privacy-first: 95%+ local processing
 * - Streaming responses via Kotlin Flow
 * - Memory-optimized for <512MB devices
 * - Battery-aware inference management
 * - Constitutional AI alignment (Phase 3)
 *
 * Architecture:
 * - Uses TVM runtime for model execution
 * - OpenAI-compatible API for easy integration
 * - Async/await pattern for non-blocking inference
 * - Thread-safe state management
 *
 * Created: 2025-10-30
 * Author: AVA AI Team
 * License: Proprietary (Augmentalis Inc.)
 */

package com.augmentalis.ava.features.llm.alc

import android.content.Context
import com.augmentalis.ava.core.common.Result
import com.augmentalis.ava.features.llm.domain.ChatMessage
import com.augmentalis.ava.features.llm.domain.LLMResponse
import com.augmentalis.ava.features.llm.domain.TokenUsage
import kotlinx.coroutines.CoroutineDispatcher
import kotlinx.coroutines.CoroutineScope
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.SupervisorJob
import kotlinx.coroutines.channels.Channel
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.flow
import kotlinx.coroutines.launch
import kotlinx.coroutines.sync.Mutex
import kotlinx.coroutines.sync.withLock
import timber.log.Timber
import java.util.concurrent.atomic.AtomicBoolean
import java.util.concurrent.atomic.AtomicInteger

/**
 * Main ALC Engine for on-device LLM inference
 */
class ALCEngine(
    private val context: Context,
    private val dispatcher: CoroutineDispatcher = Dispatchers.Default
) {
    // Engine state
    private val engineState = AtomicInteger(EngineState.UNINITIALIZED.ordinal)
    private val isGenerating = AtomicBoolean(false)

    // Thread safety
    private val engineMutex = Mutex()
    private val scope = CoroutineScope(dispatcher + SupervisorJob())

    // TVM runtime components
    private var tvmRuntime: TVMRuntime? = null
    private var modelModule: TVMModule? = null

    // Model configuration
    private var modelConfig: ModelConfig? = null

    // Generation state
    private var currentRequestId = AtomicInteger(0)
    private val activeStreams = mutableMapOf<Int, Channel<String>>()

    // Performance tracking
    private var totalTokensGenerated = 0L
    private var totalInferenceTimeMs = 0L

    /**
     * Initialize the ALC Engine with model configuration
     */
    suspend fun initialize(config: ModelConfig): Result<Unit> = engineMutex.withLock {
        if (engineState.get() != EngineState.UNINITIALIZED.ordinal) {
            return Result.Error(
                message = "Engine already initialized",
                exception = IllegalStateException("Current state: ${getCurrentState()}")
            )
        }

        try {
            Timber.d("Initializing ALC Engine with model: ${config.modelPath}")
            engineState.set(EngineState.INITIALIZING.ordinal)

            // Initialize TVM runtime
            tvmRuntime = TVMRuntime.create(context)

            // Load model module
            modelModule = tvmRuntime?.loadModule(
                modelPath = config.modelPath,
                modelLib = config.modelLib,
                device = config.device
            )

            // Store configuration
            modelConfig = config

            // Verify model loaded correctly
            if (modelModule == null) {
                throw IllegalStateException("Failed to load model module")
            }

            engineState.set(EngineState.READY.ordinal)
            Timber.i("ALC Engine initialized successfully")

            Result.Success(Unit)
        } catch (e: Exception) {
            engineState.set(EngineState.ERROR.ordinal)
            Timber.e(e, "Failed to initialize ALC Engine")
            Result.Error(
                message = "Initialization failed: ${e.message}",
                exception = e
            )
        }
    }

    /**
     * Generate a streaming response for a chat conversation
     */
    fun chat(
        messages: List<ChatMessage>,
        options: GenerationOptions = GenerationOptions()
    ): Flow<LLMResponse> = flow {
        // Validate state
        if (engineState.get() != EngineState.READY.ordinal) {
            emit(LLMResponse.Error(
                message = "Engine not ready. Current state: ${getCurrentState()}",
                code = "ENGINE_NOT_READY"
            ))
            return@flow
        }

        // Check if already generating
        if (!isGenerating.compareAndSet(false, true)) {
            emit(LLMResponse.Error(
                message = "Engine is currently generating another response",
                code = "ENGINE_BUSY"
            ))
            return@flow
        }

        try {
            // Create unique request ID
            val requestId = currentRequestId.incrementAndGet()
            Timber.d("Starting chat generation (request $requestId)")

            // Prepare prompt from messages
            val prompt = formatMessagesAsPrompt(messages)

            // Create streaming channel
            val streamChannel = Channel<String>(capacity = Channel.BUFFERED)
            activeStreams[requestId] = streamChannel

            // Start generation in background
            scope.launch {
                try {
                    generateStreaming(
                        prompt = prompt,
                        options = options,
                        streamChannel = streamChannel,
                        requestId = requestId
                    )
                } catch (e: Exception) {
                    Timber.e(e, "Generation failed for request $requestId")
                    streamChannel.send("[ERROR] ${e.message}")
                } finally {
                    streamChannel.close()
                    activeStreams.remove(requestId)
                }
            }

            // Emit streaming chunks
            var fullText = ""
            var tokenCount = 0
            val startTime = System.currentTimeMillis()

            for (chunk in streamChannel) {
                if (chunk.startsWith("[ERROR]")) {
                    emit(LLMResponse.Error(
                        message = chunk.removePrefix("[ERROR] "),
                        code = "GENERATION_ERROR"
                    ))
                    break
                }

                fullText += chunk
                tokenCount++

                emit(LLMResponse.Streaming(
                    chunk = chunk,
                    tokenCount = tokenCount
                ))
            }

            // Emit completion
            val endTime = System.currentTimeMillis()
            val inferenceTime = endTime - startTime

            totalTokensGenerated += tokenCount
            totalInferenceTimeMs += inferenceTime

            emit(LLMResponse.Complete(
                fullText = fullText,
                usage = TokenUsage(
                    promptTokens = estimateTokenCount(prompt),
                    completionTokens = tokenCount,
                    totalTokens = estimateTokenCount(prompt) + tokenCount
                )
            ))

            Timber.i("Completed request $requestId: $tokenCount tokens in ${inferenceTime}ms")

        } catch (e: Exception) {
            Timber.e(e, "Chat generation failed")
            emit(LLMResponse.Error(
                message = "Generation failed: ${e.message}",
                code = "GENERATION_EXCEPTION",
                exception = e
            ))
        } finally {
            isGenerating.set(false)
        }
    }

    /**
     * Generate streaming response using TVM runtime
     */
    private suspend fun generateStreaming(
        prompt: String,
        options: GenerationOptions,
        streamChannel: Channel<String>,
        requestId: Int
    ) {
        val module = modelModule ?: throw IllegalStateException("Model module not loaded")
        val config = modelConfig ?: throw IllegalStateException("Model config not set")

        try {
            // Tokenize input
            val inputTokens = tokenize(prompt)
            Timber.d("Tokenized prompt: ${inputTokens.size} tokens")

            // Initialize generation state
            var generatedTokens = 0
            val maxTokens = options.maxTokens ?: config.maxGenerationTokens
            var currentTokenIds = inputTokens.toMutableList()

            // Autoregressive generation loop
            while (generatedTokens < maxTokens) {
                // Check if generation was stopped
                if (activeStreams[requestId] == null) {
                    Timber.d("Generation stopped for request $requestId")
                    break
                }

                // Run model inference
                val logits = module.forward(currentTokenIds.toIntArray())

                // Sample next token
                val nextTokenId = sampleToken(
                    logits = logits,
                    temperature = options.temperature,
                    topP = options.topP
                )

                // Check for stop sequences
                if (isStopToken(nextTokenId) || options.stopSequences.any { isStopSequence(currentTokenIds, it) }) {
                    Timber.d("Stop token/sequence detected")
                    break
                }

                // Decode token to text
                val tokenText = detokenize(listOf(nextTokenId))

                // Stream chunk
                streamChannel.send(tokenText)

                // Update state
                currentTokenIds.add(nextTokenId)
                generatedTokens++

                // Memory management: trim context if too long
                if (currentTokenIds.size > config.maxContextLength) {
                    currentTokenIds = currentTokenIds.drop(currentTokenIds.size - config.maxContextLength).toMutableList()
                }
            }

            Timber.d("Generated $generatedTokens tokens for request $requestId")

        } catch (e: Exception) {
            Timber.e(e, "Streaming generation failed")
            streamChannel.send("[ERROR] ${e.message}")
        }
    }

    /**
     * Tokenize text input to token IDs
     */
    private fun tokenize(text: String): List<Int> {
        val runtime = tvmRuntime ?: throw IllegalStateException("TVM runtime not initialized")
        return runtime.tokenize(text)
    }

    /**
     * Detokenize token IDs to text
     */
    private fun detokenize(tokenIds: List<Int>): String {
        val runtime = tvmRuntime ?: throw IllegalStateException("TVM runtime not initialized")
        return runtime.detokenize(tokenIds)
    }

    /**
     * Sample next token from logits
     */
    private fun sampleToken(
        logits: FloatArray,
        temperature: Float,
        topP: Float
    ): Int {
        if (temperature == 0f) {
            // Greedy sampling
            return logits.indices.maxByOrNull { logits[it] } ?: 0
        }

        // Apply temperature
        val scaledLogits = logits.map { it / temperature }.toFloatArray()

        // Softmax
        val expLogits = scaledLogits.map { kotlin.math.exp(it.toDouble()).toFloat() }
        val sumExp = expLogits.sum()
        val probs = expLogits.map { it / sumExp }

        // Top-p sampling
        val sortedIndices = probs.indices.sortedByDescending { probs[it] }
        var cumProb = 0f
        val topPIndices = mutableListOf<Int>()

        for (idx in sortedIndices) {
            cumProb += probs[idx]
            topPIndices.add(idx)
            if (cumProb >= topP) break
        }

        // Sample from top-p tokens
        val topPProbs = topPIndices.map { probs[it] }
        val topPSum = topPProbs.sum()
        val normalizedProbs = topPProbs.map { it / topPSum }

        val random = Math.random().toFloat()
        var accum = 0f
        for ((i, prob) in normalizedProbs.withIndex()) {
            accum += prob
            if (random <= accum) {
                return topPIndices[i]
            }
        }

        return topPIndices.last()
    }

    /**
     * Check if token is a stop token (EOS)
     */
    private fun isStopToken(tokenId: Int): Boolean {
        val config = modelConfig ?: return false
        return tokenId in config.stopTokenIds
    }

    /**
     * Check if sequence matches stop sequence
     */
    private fun isStopSequence(tokenIds: List<Int>, stopSequence: String): Boolean {
        if (tokenIds.size < 5) return false
        val recentText = detokenize(tokenIds.takeLast(10))
        return recentText.contains(stopSequence)
    }

    /**
     * Format chat messages as prompt string
     */
    private fun formatMessagesAsPrompt(messages: List<ChatMessage>): String {
        return buildString {
            for (message in messages) {
                append("<|im_start|>")
                append(message.role.toApiString())
                append("\n")
                append(message.content)
                append("<|im_end|>\n")
            }
            append("<|im_start|>assistant\n")
        }
    }

    /**
     * Estimate token count (rough approximation)
     */
    private fun estimateTokenCount(text: String): Int {
        return (text.length / 4).coerceAtLeast(1)
    }

    /**
     * Stop current generation
     */
    fun stop() {
        if (isGenerating.get()) {
            Timber.d("Stopping generation")
            activeStreams.clear()
            isGenerating.set(false)
        }
    }

    /**
     * Reset engine state
     */
    fun reset() {
        stop()
        totalTokensGenerated = 0L
        totalInferenceTimeMs = 0L
        Timber.d("Engine state reset")
    }

    /**
     * Clean up resources
     */
    suspend fun cleanup() = engineMutex.withLock {
        try {
            Timber.d("Cleaning up ALC Engine")
            stop()
            modelModule?.dispose()
            modelModule = null
            tvmRuntime?.dispose()
            tvmRuntime = null
            engineState.set(EngineState.UNINITIALIZED.ordinal)
            Timber.i("ALC Engine cleaned up successfully")
        } catch (e: Exception) {
            Timber.e(e, "Error during cleanup")
        }
    }

    /**
     * Get current engine state
     */
    fun getCurrentState(): EngineState {
        return EngineState.values()[engineState.get()]
    }

    /**
     * Check if engine is currently generating
     */
    fun isGenerating(): Boolean = isGenerating.get()

    /**
     * Get performance stats
     */
    fun getStats(): EngineStats {
        return EngineStats(
            totalTokensGenerated = totalTokensGenerated,
            totalInferenceTimeMs = totalInferenceTimeMs,
            averageTokensPerSecond = if (totalInferenceTimeMs > 0) {
                (totalTokensGenerated * 1000.0 / totalInferenceTimeMs).toFloat()
            } else 0f
        )
    }
}

/**
 * Engine state enum
 */
enum class EngineState {
    UNINITIALIZED,
    INITIALIZING,
    READY,
    GENERATING,
    ERROR
}

/**
 * Model configuration
 */
data class ModelConfig(
    val modelPath: String,
    val modelLib: String,
    val device: String = "opencl",
    val maxContextLength: Int = 2048,
    val maxGenerationTokens: Int = 512,
    val stopTokenIds: List<Int> = listOf(0, 1, 2)
)

/**
 * Generation options
 */
data class GenerationOptions(
    val temperature: Float = 0.7f,
    val maxTokens: Int? = null,
    val topP: Float = 0.95f,
    val frequencyPenalty: Float = 0.0f,
    val presencePenalty: Float = 0.0f,
    val stopSequences: List<String> = emptyList()
)

/**
 * Engine performance stats
 */
data class EngineStats(
    val totalTokensGenerated: Long,
    val totalInferenceTimeMs: Long,
    val averageTokensPerSecond: Float
)
