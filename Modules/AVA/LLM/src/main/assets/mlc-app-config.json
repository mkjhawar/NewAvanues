{
  "model_list": [
    {
      "model_id": "Phi-3.5-mini-instruct-q4f16_0-MLC",
      "model_lib": "phi3_q4f16_0_5fe42298399a05eb2a1878fdc1c8c115",
      "model_url": "https://huggingface.co/mlc-ai/Phi-3.5-mini-instruct-q4f16_0-MLC",
      "estimated_vram_bytes": 4250586449
    },
    {
      "model_id": "Qwen2.5-1.5B-Instruct-q4f16_1-MLC",
      "model_lib": "qwen2_q4f16_1_2e221f430380225c03990ad24c3d030e",
      "model_url": "https://huggingface.co/mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC",
      "estimated_vram_bytes": 3980990464
    },
    {
      "model_id": "gemma-2-2b-it-q4f16_1-MLC",
      "model_lib": "gemma2_q4f16_1_5cc7dbd3ae3d1040984d9720b2d7b7d4",
      "model_url": "https://huggingface.co/mlc-ai/gemma-2-2b-it-q4f16_1-MLC",
      "estimated_vram_bytes": 3000000000
    },
    {
      "model_id": "Llama-3.2-3B-Instruct-q4f16_0-MLC",
      "model_lib": "llama_q4f16_0_2d32572d8a4ab2af20a1f587ef6c8c63",
      "model_url": "https://huggingface.co/mlc-ai/Llama-3.2-3B-Instruct-q4f16_0-MLC",
      "estimated_vram_bytes": 4679979417
    },
    {
      "model_id": "Mistral-7B-Instruct-v0.3-q4f16_1-MLC",
      "model_lib": "mistral_q4f16_1_c2cba77a6def4dd52f7e20b5d8576ab5",
      "model_url": "https://huggingface.co/mlc-ai/Mistral-7B-Instruct-v0.3-q4f16_1-MLC",
      "estimated_vram_bytes": 4115131883
    }
  ]
}
